
@misc{noauthor_brain_nodate,
	title = {Brain {Simulation}},
	note = "{https://www.humanbrainproject.eu/en/brain-simulation/}",
	urldate = {2018-11-30},
	file = {Brain Simulation:C\:\\Users\\Caleb\\Zotero\\storage\\WKZ7NIFR\\brain-simulation.html:text/html}
}

@article{duch_survey_1999,
	title = {Survey of neural transfer functions},
	volume = {2},
	number = {1},
	journal = {Neural Computing Surveys},
	author = {Duch, W{\textbackslash}lodzis{\textbackslash}law and Jankowski, Norbert},
	year = {1999},
	pages = {163--212}
}

@misc{noauthor_survey_nodate,
	title = {Survey of {Neural} {Transfer} {Functions} - {Semantic} {Scholar}},
	url = {https://www.semanticscholar.org/paper/Survey-of-Neural-Transfer-Functions-Duch-Jankowski/50975d6cd92e71f828ffd54bf776c32daa79e295},
	urldate = {2018-11-30},
	file = {Survey of Neural Transfer Functions - Semantic Scholar:C\:\\Users\\Caleb\\Zotero\\storage\\JCAUVXU2\\50975d6cd92e71f828ffd54bf776c32daa79e295.html:text/html}
}

@misc{noauthor_build_nodate,
	title = {Build with {AI}},
	note = "{https://deepai.org/machine-learning-glossary-and-terms/neural-network}",
	abstract = {Leverage the power of artificial intelligence in your applications with our image and video recognition API.},
	urldate = {2018-11-30},
	journal = {DeepAI},
	file = {Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\8DYANNXK\\neural-network.html:text/html}
}

@inproceedings{long_review_2010,
	address = {Atlanta, Georgia},
	title = {A {Review} of {Biologically} {Plausible} {Neuron} {Models} for {Spiking} {Neural} {Networks}},
	isbn = {978-1-60086-963-1},
	url = {http://arc.aiaa.org/doi/10.2514/6.2010-3540},
	doi = {10.2514/6.2010-3540},
	language = {en},
	urldate = {2018-11-30},
	booktitle = {{AIAA} {Infotech}@{Aerospace} 2010},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Long, Lyle and Fang, Guoliang},
	month = apr,
	year = {2010},
	file = {Long and Fang - 2010 - A Review of Biologically Plausible Neuron Models f.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\NTXLTE28\\Long and Fang - 2010 - A Review of Biologically Plausible Neuron Models f.pdf:application/pdf}
}

@article{eliasmith_use_2014,
	title = {The use and abuse of large-scale brain models},
	volume = {25},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095943881300189X},
	doi = {10.1016/j.conb.2013.09.009},
	language = {en},
	urldate = {2018-11-30},
	journal = {Current Opinion in Neurobiology},
	author = {Eliasmith, Chris and Trujillo, Oliver},
	month = apr,
	year = {2014},
	pages = {1--6},
	file = {Eliasmith and Trujillo - 2014 - The use and abuse of large-scale brain models.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\MSJGMBCH\\Eliasmith and Trujillo - 2014 - The use and abuse of large-scale brain models.pdf:application/pdf}
}

@article{hassabis_neuroscience-inspired_2017,
	title = {Neuroscience-{Inspired} {Artificial} {Intelligence}},
	volume = {95},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317305093},
	doi = {10.1016/j.neuron.2017.06.011},
	language = {en},
	number = {2},
	urldate = {2018-11-30},
	journal = {Neuron},
	author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	month = jul,
	year = {2017},
	pages = {245--258},
	file = {Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\8FS6FPFZ\\Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:application/pdf}
}

@article{hassabis_neuroscience-inspired_2017-1,
	title = {Neuroscience-{Inspired} {Artificial} {Intelligence}},
	volume = {95},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317305093},
	doi = {10.1016/j.neuron.2017.06.011},
	language = {en},
	number = {2},
	urldate = {2018-11-30},
	journal = {Neuron},
	author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	month = jul,
	year = {2017},
	pages = {245--258},
	file = {Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\E4QEFMY2\\Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:application/pdf}
}

@article{marblestone_towards_2016,
	title = {Towards an integration of deep learning and neuroscience},
	url = {http://arxiv.org/abs/1606.03813},
	abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
	urldate = {2018-11-30},
	journal = {arXiv:1606.03813 [q-bio]},
	author = {Marblestone, Adam and Wayne, Greg and Kording, Konrad},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03813},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv\:1606.03813 PDF:C\:\\Users\\Caleb\\Zotero\\storage\\Q5I9AFZ2\\Marblestone et al. - 2016 - Towards an integration of deep learning and neuros.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\622VH9Z6\\1606.html:text/html}
}

@article{cleland_computation_2005,
	title = {Computation in the {Olfactory} {System}},
	volume = {30},
	issn = {0379-864X},
	url = {https://academic.oup.com/chemse/article/30/9/801/287465},
	doi = {10.1093/chemse/bji072},
	abstract = {Abstract.  Computational models are increasingly essential to systems neuroscience. Models serve as proofs of concept, tests of sufficiency, and as quantitative},
	language = {en},
	number = {9},
	urldate = {2018-11-30},
	journal = {Chemical Senses},
	author = {Cleland, Thomas A. and Linster, Christiane},
	month = nov,
	year = {2005},
	pages = {801--813},
	file = {Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\C4BP7KUL\\Cleland and Linster - 2005 - Computation in the Olfactory System.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\M9JJG7Y2\\287465.html:text/html}
}

@article{singer_neuronal_1999,
	title = {Neuronal {Synchrony}: {A} {Versatile} {Code} for the {Definition} of {Relations}?},
	volume = {24},
	issn = {0896-6273},
	shorttitle = {Neuronal {Synchrony}},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(00)80821-1},
	doi = {10.1016/S0896-6273(00)80821-1},
	language = {English},
	number = {1},
	urldate = {2018-11-30},
	journal = {Neuron},
	author = {Singer, Wolf},
	month = sep,
	year = {1999},
	pmid = {10677026},
	pages = {49--65},
	file = {Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\8HQKUJTM\\Singer - 1999 - Neuronal Synchrony A Versatile Code for the Defin.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\S3G34RZL\\S0896-6273(00)80821-1.html:text/html}
}

@article{huys_computational_2016,
	title = {Computational psychiatry as a bridge from neuroscience to clinical applications},
	volume = {19},
	copyright = {2016 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4238},
	doi = {10.1038/nn.4238},
	abstract = {Translating advances in neuroscience into benefits for patients with mental illness presents enormous challenges because it involves both the most complex organ, the brain, and its interaction with a similarly complex environment. Dealing with such complexities demands powerful techniques. Computational psychiatry combines multiple levels and types of computation with multiple types of data in an effort to improve understanding, prediction and treatment of mental illness. Computational psychiatry, broadly defined, encompasses two complementary approaches: data driven and theory driven. Data-driven approaches apply machine-learning methods to high-dimensional data to improve classification of disease, predict treatment outcomes or improve treatment selection. These approaches are generally agnostic as to the underlying mechanisms. Theory-driven approaches, in contrast, use models that instantiate prior knowledge of, or explicit hypotheses about, such mechanisms, possibly at multiple levels of analysis and abstraction. We review recent advances in both approaches, with an emphasis on clinical applications, and highlight the utility of combining them.},
	language = {en},
	number = {3},
	urldate = {2018-11-30},
	journal = {Nature Neuroscience},
	author = {Huys, Quentin J. M. and Maia, Tiago V. and Frank, Michael J.},
	month = mar,
	year = {2016},
	pages = {404--413},
	file = {Accepted Version:C\:\\Users\\Caleb\\Zotero\\storage\\WEULH3MA\\Huys et al. - 2016 - Computational psychiatry as a bridge from neurosci.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\GWSHRTR7\\nn.html:text/html}
}

@article{huys_are_2011,
	title = {Are computational models of any use to psychiatry?},
	volume = {24},
	issn = {08936080},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608011000852},
	doi = {10.1016/j.neunet.2011.03.001},
	abstract = {Mathematically rigorous descriptions of key hypotheses and theories are becoming more common in neuroscience and are beginning to be applied to psychiatry. In this article two fictional characters, Dr. Strong and Mr. Micawber, debate the use of such computational models (CMs) in psychiatry. We present four fundamental challenges to the use of CMs in psychiatry: (a) the applicability of mathematical approaches to core concepts in psychiatry such as subjective experiences, conflict and suffering; (b) whether psychiatry is mature enough to allow informative modelling; (c) whether theoretical techniques are powerful enough to approach psychiatric problems; and (d) the issue of communicating clinical concepts to theoreticians and vice versa. We argue that CMs have yet to influence psychiatric practice, but that they help psychiatric research in two fundamental ways: (a) to build better theories integrating psychiatry with neuroscience; and (b) to enforce explicit, global and efficient testing of hypotheses through more powerful analytical methods. CMs allow the complexity of a hypothesis to be rigorously weighed against the complexity of the data. The paper concludes with a discussion of the path ahead. It points to stumbling blocks, like the poor communication between theoretical and medical communities. But it also identifies areas in which the contributions of CMs will likely be pivotal, like an understanding of social influences in psychiatry, and of the co-morbidity structure of psychiatric diseases.},
	language = {en},
	number = {6},
	urldate = {2018-11-30},
	journal = {Neural Networks},
	author = {Huys, Quentin J.M. and Moutoussis, Michael and Williams, Jonathan},
	month = aug,
	year = {2011},
	pages = {544--551},
	file = {Huys et al. - 2011 - Are computational models of any use to psychiatry.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\TMHLJXHW\\Huys et al. - 2011 - Are computational models of any use to psychiatry.pdf:application/pdf}
}

@article{adaszewski_how_2013,
	title = {How early can we predict {Alzheimer}'s disease using computational anatomy?},
	volume = {34},
	issn = {0197-4580},
	url = {http://www.sciencedirect.com/science/article/pii/S0197458013002704},
	doi = {10.1016/j.neurobiolaging.2013.06.015},
	abstract = {Computational anatomy with magnetic resonance imaging (MRI) is well established as a noninvasive biomarker of Alzheimer's disease (AD); however, there is less certainty about its dependency on the staging of AD. We use classical group analyses and automated machine learning classification of standard structural MRI scans to investigate AD diagnostic accuracy from the preclinical phase to clinical dementia. Longitudinal data from the Alzheimer's Disease Neuroimaging Initiative were stratified into 4 groups according to the clinical status—(1) AD patients; (2) mild cognitive impairment (MCI) converters; (3) MCI nonconverters; and (4) healthy controls—and submitted to a support vector machine. The obtained classifier was significantly above the chance level (62\%) for detecting AD already 4 years before conversion from MCI. Voxel-based univariate tests confirmed the plausibility of our findings detecting a distributed network of hippocampal-temporoparietal atrophy in AD patients. We also identified a subgroup of control subjects with brain structure and cognitive changes highly similar to those observed in AD. Our results indicate that computational anatomy can detect AD substantially earlier than suggested by current models. The demonstrated differential spatial pattern of atrophy between correctly and incorrectly classified AD patients challenges the assumption of a uniform pathophysiological process underlying clinically identified AD.},
	number = {12},
	urldate = {2018-11-30},
	journal = {Neurobiology of Aging},
	author = {Adaszewski, Stanisław and Dukart, Juergen and Kherif, Ferath and Frackowiak, Richard and Draganski, Bogdan},
	month = dec,
	year = {2013},
	keywords = {Alzheimer's disease, Biomarker, Mild cognitive impairment, Structural magnetic resonance imaging},
	pages = {2815--2826},
	file = {ScienceDirect Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\ARI794Q6\\S0197458013002704.html:text/html}
}

@misc{noauthor_computational_nodate,
	title = {Computational neuroscience - {Latest} research and news {\textbar} {Nature}},
	url = {https://www.nature.com/subjects/computational-neuroscience},
	urldate = {2018-11-30},
	file = {Computational neuroscience - Latest research and news | Nature:C\:\\Users\\Caleb\\Zotero\\storage\\PYT2LRZU\\computational-neuroscience.html:text/html}
}

@incollection{pecevski_theoretical_2008,
	title = {Theoretical {Analysis} of {Learning} with {Reward}-{Modulated} {Spike}-{Timing}-{Dependent} {Plasticity}},
	url = {http://papers.nips.cc/paper/3349-theoretical-analysis-of-learning-with-reward-modulated-spike-timing-dependent-plasticity.pdf},
	urldate = {2018-11-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Pecevski, Dejan and Maass, Wolfgang and Legenstein, Robert A.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {881--888},
	file = {NIPS Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\VHEHSUHU\\Pecevski et al. - 2008 - Theoretical Analysis of Learning with Reward-Modul.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\7UB8HEA2\\3349-theoretical-analysis-of-learning-with-reward-modulated-spike-timing-dependent-plasticity.html:text/html}
}

@article{gallicchio_deep_2016,
	title = {Deep {Reservoir} {Computing}: {A} {Critical} {Analysis}},
	abstract = {In this paper we propose an empirical analysis of deep recurrent neural networks (RNNs) with stacked layers. The analysis aims at the study and proposal of approaches to develop and enhance multiple timescale and hierarchical dynamics in deep recurrent architectures, within the eﬃcient Reservoir Computing (RC) approach for RNN modeling. Results point out the actual relevance of layering and RC parameters aspects on the diversiﬁcation of temporal representations in deep recurrent models.},
	language = {en},
	journal = {Computational Intelligence},
	author = {Gallicchio, Claudio and Micheli, Alessio},
	year = {2016},
	pages = {6},
	file = {Gallicchio and Micheli - 2016 - Deep Reservoir Computing A Critical Analysis.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\LQ4IV84H\\Gallicchio and Micheli - 2016 - Deep Reservoir Computing A Critical Analysis.pdf:application/pdf}
}

@article{tanaka_recent_2018,
	title = {Recent {Advances} in {Physical} {Reservoir} {Computing}: {A} {Review}},
	shorttitle = {Recent {Advances} in {Physical} {Reservoir} {Computing}},
	url = {http://arxiv.org/abs/1808.04962},
	abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for extracting features of the inputs. Further, training is carried out only in the readout. Thus, the major advantage of reservoir computing is fast and simple learning compared to other recurrent neural networks. Another advantage is that the reservoir can be realized using physical systems, substrates, and devices, instead of recurrent neural networks. In fact, such physical reservoir computing has attracted increasing attention in various fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
	urldate = {2018-11-27},
	journal = {arXiv:1808.04962 [cs]},
	author = {Tanaka, Gouhei and Yamane, Toshiyuki and Héroux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.04962},
	keywords = {Computer Science - Emerging Technologies},
	annote = {Comment: 54 pages, 13 figures},
	file = {arXiv\:1808.04962 PDF:C\:\\Users\\Caleb\\Zotero\\storage\\4YDK4PY9\\Tanaka et al. - 2018 - Recent Advances in Physical Reservoir Computing A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\AJXBAFVU\\1808.html:text/html}
}

@article{wang_learning_2016,
	title = {Learning to reinforcement learn},
	url = {http://arxiv.org/abs/1611.05763},
	abstract = {In recent years deep reinforcement learning (RL) systems have attained superhuman performance in a number of challenging task domains. However, a major limitation of such applications is their demand for massive amounts of training data. A critical present objective is thus to develop deep RL methods that can adapt rapidly to new tasks. In the present work we introduce a novel approach to this challenge, which we refer to as deep meta-reinforcement learning. Previous work has shown that recurrent networks can support meta-learning in a fully supervised context. We extend this approach to the RL setting. What emerges is a system that is trained using one RL algorithm, but whose recurrent dynamics implement a second, quite separate RL procedure. This second, learned RL algorithm can differ from the original one in arbitrary ways. Importantly, because it is learned, it is configured to exploit structure in the training domain. We unpack these points in a series of seven proof-of-concept experiments, each of which examines a key aspect of deep meta-RL. We consider prospects for extending and scaling up the approach, and also point out some potentially important implications for neuroscience.},
	urldate = {2018-11-27},
	journal = {arXiv:1611.05763 [cs, stat]},
	author = {Wang, Jane X. and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.05763},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 7 figures, 1 table},
	file = {arXiv\:1611.05763 PDF:C\:\\Users\\Caleb\\Zotero\\storage\\QH9XVHJV\\Wang et al. - 2016 - Learning to reinforcement learn.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\TJI5L6J7\\1611.html:text/html}
}

@misc{noauthor_mit_nodate,
	title = {{MIT} {Press} {Journals}},
	url = {https://www.mitpressjournals.org/},
	abstract = {MIT Press Journals is a mission-driven, not-for-profit scholarly publisher devoted to the widest dissemination of its content.},
	language = {en},
	urldate = {2018-11-27},
	journal = {MIT Press Journals},
	file = {Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\DFXDVYAM\\neco.2007.19.6.html:text/html}
}

@article{bengio_towards_2015,
	title = {Towards {Biologically} {Plausible} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1502.04156},
	abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
	urldate = {2018-11-27},
	journal = {arXiv:1502.04156 [cs]},
	author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.04156},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1502.04156 PDF:C\:\\Users\\Caleb\\Zotero\\storage\\PSDBMISV\\Bengio et al. - 2015 - Towards Biologically Plausible Deep Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\2IE73ATZ\\1502.html:text/html}
}

@article{zhu_drosophila_2013,
	title = {The {Drosophila} visual system: {From} neural circuits to behavior},
	volume = {7},
	shorttitle = {The {Drosophila} visual system},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3739809/},
	doi = {10.4161/cam.25521},
	abstract = {A compact genome and a tiny brain make Drosophila the prime model to understand the neural substrate of behavior. The neurogenetic efforts to reveal neural circuits underlying Drosophila vision started about half a century ago, and now the field is booming ...},
	language = {en},
	number = {4},
	urldate = {2018-11-27},
	journal = {Cell Adhesion \& Migration},
	author = {Zhu, Yan},
	month = jul,
	year = {2013},
	pmid = {23880926},
	pages = {333},
	file = {Full Text:C\:\\Users\\Caleb\\Zotero\\storage\\KDL5QF5D\\Zhu - 2013 - The Drosophila visual system From neural circuits.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\S5T3X5FQ\\PMC3739809.html:text/html}
}

@article{jaeger_tutorial_nodate,
	title = {A tutorial on training recurrent neural networks, covering {BPPT}, {RTRL}, {EKF} and the "echo state network" approach},
	abstract = {This tutorial is a worked-out version of a 5-hour course originally held at AIS in September/October 2002. It has two distinct components. First, it contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time (BPTT), real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF). This material is covered in Sections 2 – 5. The remaining sections 1 and 6 – 9 are much more gentle, more detailed, and illustrated with simple examples. They are intended to be useful as a stand-alone tutorial for the echo state network (ESN) approach to recurrent neural network training.},
	language = {en},
	author = {Jaeger, Herbert},
	pages = {46},
	file = {Jaeger - A tutorial on training recurrent neural networks, .pdf:C\:\\Users\\Caleb\\Zotero\\storage\\UQPAQZ78\\Jaeger - A tutorial on training recurrent neural networks, .pdf:application/pdf}
}

@article{norton_preparing_nodate,
	title = {Preparing {More} {Effective} {Liquid} {State} {Machines} {Using} {Hebbian} {Learning}},
	abstract = {In Liquid State Machines, separation is a critical attribute of the liquid—which is traditionally not trained. The effects of using Hebbian learning in the liquid to improve separation are investigated in this paper. When presented with random input, Hebbian learning does not dramatically change separation. However, Hebbian learning does improve separation when presented with real-world speech data.},
	language = {en},
	author = {Norton, David and Ventura, Dan},
	pages = {6},
	file = {Norton and Ventura - Preparing More Effective Liquid State Machines Usi.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\2DMNCTBG\\Norton and Ventura - Preparing More Effective Liquid State Machines Usi.pdf:application/pdf}
}

@article{jaeger_echo_nodate,
	title = {The “echo state” approach to analysing and training recurrent neural networks – with an {Erratum} note},
	language = {en},
	author = {Jaeger, Herbert},
	pages = {48},
	file = {Jaeger - The “echo state” approach to analysing and trainin.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\P66QP4GT\\Jaeger - The “echo state” approach to analysing and trainin.pdf:application/pdf}
}

@article{simoncelli_natural_2001,
	title = {Natural {Image} {Statistics} and {Neural} {Representation}},
	volume = {24},
	issn = {0147-006X, 1545-4126},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.neuro.24.1.1193},
	doi = {10.1146/annurev.neuro.24.1.1193},
	language = {en},
	number = {1},
	urldate = {2018-11-27},
	journal = {Annual Review of Neuroscience},
	author = {Simoncelli, Eero P and Olshausen, Bruno A},
	month = mar,
	year = {2001},
	pages = {1193--1216},
	file = {Simoncelli and Olshausen - 2001 - Natural Image Statistics and Neural Representation.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\DC2WHRJK\\Simoncelli and Olshausen - 2001 - Natural Image Statistics and Neural Representation.pdf:application/pdf}
}

@article{oppenheim_importance_1981,
	title = {The importance of phase in signals},
	volume = {69},
	issn = {0018-9219},
	doi = {10.1109/PROC.1981.12022},
	abstract = {In the Fourier representation of signals, spectral magnitude and phase tend to play different roles and in some situations many of the important features of a signal are preserved if only the phase is retained. Furthermore, under a variety of conditions, such as when a signal is of finite length, phase information alone is sufficient to completely reconstruct a signal to within a scale factor. In this paper, we review and discuss these observations and results in a number of different contexts and applications. Specifically, the intelligibility of phase-only reconstruction for images, speech, and crystallographic structures are illustrated. Several approaches to justifying the relative importance of phase through statistical arguments are presented, along with a number of informal arguments suggesting reasons for the importance of phase. Specific conditions under which a sequence can be exactly reconstructed from phase are reviewed, both for one-dimensional and multi-dimensional sequences, and algorithms for both approximate and exact reconstruction of signals from phase information are presented. A number of applications of the observations and results in this paper are suggested.},
	number = {5},
	journal = {Proceedings of the IEEE},
	author = {Oppenheim, A. V. and Lim, J. S.},
	month = may,
	year = {1981},
	keywords = {Acoustic scattering, Crystallography, Fourier transforms, Image reconstruction, Optical recording, Optical scattering, Prognostics and health management, Speech, X-ray diffraction, X-ray scattering},
	pages = {529--541},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Caleb\\Zotero\\storage\\UILJPXYI\\1456290.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\RK8QQEKB\\Oppenheim and Lim - 1981 - The importance of phase in signals.pdf:application/pdf}
}

@article{kiselev_molecular_2000,
	title = {A {Molecular} {Pathway} for {Light}-{Dependent} {Photoreceptor} {Apoptosis} in {Drosophila}},
	volume = {28},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(00)00092-1},
	doi = {10.1016/S0896-6273(00)00092-1},
	language = {English},
	number = {1},
	urldate = {2018-11-24},
	journal = {Neuron},
	author = {Kiselev, Alexander and Socolich, Michael and Vinós, Javier and Hardy, Robert W. and Zuker, Charles S. and Ranganathan, Rama},
	month = oct,
	year = {2000},
	pmid = {11086990},
	pages = {139--152},
	file = {Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\Y92P7BZQ\\Kiselev et al. - 2000 - A Molecular Pathway for Light-Dependent Photorecep.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\UHM5D4SD\\S0896-6273(00)00092-1.html:text/html}
}

@article{jeibmann_drosophila_2009,
	title = {Drosophila melanogaster as a {Model} {Organism} of {Brain} {Diseases}},
	volume = {10},
	issn = {1422-0067},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2660653/},
	doi = {10.3390/ijms10020407},
	abstract = {Drosophila melanogaster has been utilized to model human brain diseases. In most of these invertebrate transgenic models, some aspects of human disease are reproduced. Although investigation of rodent models has been of significant impact, invertebrate models offer a wide variety of experimental tools that can potentially address some of the outstanding questions underlying neurological disease. This review considers what has been gleaned from invertebrate models of neurodegenerative diseases, including Alzheimer’s disease, Parkinson’s disease, metabolic diseases such as Leigh disease, Niemann-Pick disease and ceroid lipofuscinoses, tumor syndromes such as neurofibromatosis and tuberous sclerosis, epilepsy as well as CNS injury. It is to be expected that genetic tools in Drosophila will reveal new pathways and interactions, which hopefully will result in molecular based therapy approaches.},
	number = {2},
	urldate = {2018-11-24},
	journal = {International Journal of Molecular Sciences},
	author = {Jeibmann, Astrid and Paulus, Werner},
	month = feb,
	year = {2009},
	pmid = {19333415},
	pmcid = {PMC2660653},
	pages = {407--440},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\2B69C8FW\\Jeibmann and Paulus - 2009 - Drosophila melanogaster as a Model Organism of Bra.pdf:application/pdf}
}

@article{fang_temporal_2008,
	series = {17th {IFAC} {World} {Congress}},
	title = {Temporal pattern recognition using spiking neural networks for cortical neuronal spike train decoding},
	volume = {41},
	issn = {1474-6670},
	url = {http://www.sciencedirect.com/science/article/pii/S1474667016397695},
	doi = {10.3182/20080706-5-KR-1001.00874},
	abstract = {Most experimental and decoding algorithm studies of brain neural signals assume that neurons transmit information as a rate coding, but recent studies on the fast cortical computations indicate that temporal coding is probably a more biologically plausible scheme used by neurons. We introduce spiking neural networks (SNN) which consist of spiking neurons propagate information by the timing of spikes to analyze the cortical neural spike trains directly without temporal information lost. The SNN based temporal pattern classification is compared with the conventional artificial neural networks (ANN) based firing rate analysis. The results show that the SNN algorithm can achieve higher accuracy, which demonstrates that temporal coding is a viable code for fast neural information processing and the SNN approach is suitable for recognizing the temporal pattern in the cortical neural signals.},
	number = {2},
	urldate = {2018-11-08},
	journal = {IFAC Proceedings Volumes},
	author = {Fang, Huijuan and Wang, Yongji and He, Jiping and Liu, Shan},
	month = jan,
	year = {2008},
	pages = {5203--5208},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\KTY5XBTT\\Fang et al. - 2008 - Temporal pattern recognition using spiking neural .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\4P3DXJBT\\S1474667016397695.html:text/html}
}

@article{maass_computational_2004,
	title = {On the computational power of circuits of spiking neurons},
	volume = {69},
	issn = {00220000},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0022000004000406},
	doi = {10.1016/j.jcss.2004.04.001},
	abstract = {Complex real-time computations on multi-modal time-varying input streams are carried out by generic cortical microcircuits. Obstacles for the development of adequate theoretical models that could explain the seemingly universal power of cortical microcircuits for real-time computing are the complexity and diversity of their computational units (neurons and synapses), as well as the traditional emphasis on ofﬂine computing in almost all theoretical approaches towards neural computation. In this article, we initiate a rigorous mathematical analysis of the real-time computing capabilities of a new generation of models for neural computation, liquid state machines, that can be implemented with—in fact beneﬁt from—diverse computational units. Hence, realistic models for cortical microcircuits represent special instances of such liquid state machines, without any need to simplify or homogenize their diverse computational units. We present proofs of two theorems about the potential computational power of such models for real-time computing, both on analog input streams and for spike trains as inputs.},
	language = {en},
	number = {4},
	urldate = {2018-11-08},
	journal = {Journal of Computer and System Sciences},
	author = {Maass, Wolfgang and Markram, Henry},
	month = dec,
	year = {2004},
	pages = {593--616},
	file = {Maass and Markram - 2004 - On the computational power of circuits of spiking .pdf:C\:\\Users\\Caleb\\Zotero\\storage\\SZ2TGJFF\\Maass and Markram - 2004 - On the computational power of circuits of spiking .pdf:application/pdf}
}

@article{yamazaki_cerebellum_2007,
	series = {Echo {State} {Networks} and {Liquid} {State} {Machines}},
	title = {The cerebellum as a liquid state machine},
	volume = {20},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608007000366},
	doi = {10.1016/j.neunet.2007.04.004},
	abstract = {We examined closely the cerebellar circuit model that we have proposed previously. The model granular layer generates a finite but very long sequence of active neuron populations without recurrence, which is able to represent the passage of time. For all the possible binary patterns fed into mossy fibres, the circuit generates the same number of different sequences of active neuron populations. Model Purkinje cells that receive parallel fiber inputs from neurons in the granular layer learn to stop eliciting spikes at the timing instructed by the arrival of signals from the inferior olive. These functional roles of the granular layer and Purkinje cells are regarded as a liquid state generator and readout neurons, respectively. Thus, the cerebellum that has been considered to date as a biological counterpart of a perceptron is reinterpreted to be a liquid state machine that possesses powerful information processing capability more than a perceptron.},
	number = {3},
	urldate = {2018-11-08},
	journal = {Neural Networks},
	author = {Yamazaki, Tadashi and Tanaka, Shigeru},
	month = apr,
	year = {2007},
	keywords = {Cerebellum, Hybrid network, Liquid state machine, Long-term depression, Perceptron, Recurrent inhibitory network, Sparse coding, Spatiotemporal activity patterns},
	pages = {290--297},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\I7IQNGLH\\Yamazaki and Tanaka - 2007 - The cerebellum as a liquid state machine.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\S3YHG3JG\\S0893608007000366.html:text/html}
}

@incollection{nikolic_temporal_2007,
	title = {Temporal dynamics of information content carried by neurons in the primary visual cortex},
	url = {http://papers.nips.cc/paper/3096-temporal-dynamics-of-information-content-carried-by-neurons-in-the-primary-visual-cortex.pdf},
	urldate = {2018-11-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Nikolić, Danko and Haeusler, Stefan and Singer, Wolf and Maass, Wolfgang},
	editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
	year = {2007},
	pages = {1041--1048},
	file = {NIPS Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\2YYST7QM\\Nikolić et al. - 2007 - Temporal dynamics of information content carried b.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\VP2RCNEM\\3096-temporal-dynamics-of-information-content-carried-by-neurons-in-the-primary-visual-cortex.html:text/html}
}

@article{bekolay_simultaneous_nodate,
	title = {Simultaneous unsupervised and supervised learning of cognitive functions in biologically plausible spiking neural networks},
	abstract = {We present a novel learning rule for learning transformations of sophisticated neural representations in a biologically plausible manner. We show that the rule, which uses only information available locally to a synapse in a spiking network, can learn to transmit and bind semantic pointers. Semantic pointers have previously been used to build Spaun, which is currently the world’s largest functional brain model (Eliasmith et al., 2012). Two operations commonly performed by Spaun are semantic pointer binding and transmission. It has not yet been shown how the binding and transmission operations can be learned. The learning rule combines a previously proposed supervised learning rule and a novel spiking form of the BCM unsupervised learning rule. We show that spiking BCM increases sparsity of connection weights at the cost of increased signal transmission error. We also demonstrate that the combined learning rule can learn transformations as well as the supervised rule and the ofﬂine optimization used previously. We also demonstrate that the combined learning rule is more robust to changes in parameters and leads to better outcomes in higher dimensional spaces, which is critical for explaining cognitive performance on diverse tasks.},
	language = {en},
	author = {Bekolay, Trevor and Kolbeck, Carter and Eliasmith, Chris},
	pages = {7},
	file = {Bekolay et al. - Simultaneous unsupervised and supervised learning .pdf:C\:\\Users\\Caleb\\Zotero\\storage\\HWHYNVN9\\Bekolay et al. - Simultaneous unsupervised and supervised learning .pdf:application/pdf}
}

@article{zhang_digital_2015,
	title = {A {Digital} {Liquid} {State} {Machine} {With} {Biologically} {Inspired} {Learning} and {Its} {Application} to {Speech} {Recognition}},
	volume = {26},
	issn = {2162-237X},
	doi = {10.1109/TNNLS.2015.2388544},
	abstract = {This paper presents a bioinspired digital liquid-state machine (LSM) for low-power very-large-scale-integration (VLSI)-based machine learning applications. To the best of the authors' knowledge, this is the first work that employs a bioinspired spike-based learning algorithm for the LSM. With the proposed online learning, the LSM extracts information from input patterns on the fly without needing intermediate data storage as required in offline learning methods such as ridge regression. The proposed learning rule is local such that each synaptic weight update is based only upon the firing activities of the corresponding presynaptic and postsynaptic neurons without incurring global communications across the neural network. Compared with the backpropagation-based learning, the locality of computation in the proposed approach lends itself to efficient parallel VLSI implementation. We use subsets of the TI46 speech corpus to benchmark the bioinspired digital LSM. To reduce the complexity of the spiking neural network model without performance degradation for speech recognition, we study the impacts of synaptic models on the fading memory of the reservoir and hence the network performance. Moreover, we examine the tradeoffs between synaptic weight resolution, reservoir size, and recognition performance and present techniques to further reduce the overhead of hardware implementation. Our simulation results show that in terms of isolated word recognition evaluated using the TI46 speech corpus, the proposed digital LSM rivals the state-of-the-art hidden Markov-model-based recognizer Sphinx-4 and outperforms all other reported recognizers including the ones that are based upon the LSM or neural networks.},
	number = {11},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Zhang, Y. and Li, P. and Jin, Y. and Choe, Y.},
	month = nov,
	year = {2015},
	keywords = {Neurons, Speech, Action Potentials, Auditory Pathways, backpropagation, backpropagation-based learning, bioinspired digital LSM, bioinspired spike-based learning algorithm, Biological neural networks, data storage, digital liquid state machine, finite state machines, Hardware implementation, hidden Markov models, Hidden Markov models, Humans, isolated word recognition, liquid-state machine (LSM), Machine Learning, Markov-model-based recognizer, Models, Neurological, Motivation, neural nets, Neural Networks (Computer), parallel VLSI implementation, Pattern Recognition, Physiological, postsynaptic neuron, presynaptic neuron, Reservoirs, speech recognition, Speech recognition, Sphinx-4, spike-based learning, spike-based learning., spiking neural network model, synaptic weight resolution, TI46 speech corpus, very-large-scale- integration-based machine learning, VLSI, VLSI-based machine learning},
	pages = {2635--2649},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Caleb\\Zotero\\storage\\9IIP4NDV\\7024132.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\2X94KR9B\\Zhang et al. - 2015 - A Digital Liquid State Machine With Biologically I.pdf:application/pdf}
}

@article{mazzoni_more_1991,
	title = {A more biologically plausible learning rule for neural networks.},
	volume = {88},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/88/10/4433},
	doi = {10.1073/pnas.88.10.4433},
	abstract = {Many recent studies have used artificial neural network algorithms to model how the brain might process information. However, back-propagation learning, the method that is generally used to train these networks, is distinctly "unbiological." We describe here a more biologically plausible learning rule, using reinforcement learning, which we have applied to the problem of how area 7a in the posterior parietal cortex of monkeys might represent visual space in head-centered coordinates. The network behaves similarly to networks trained by using back-propagation and to neurons recorded in area 7a. These results show that a neural network does not require back propagation to acquire biologically interesting properties.},
	language = {en},
	number = {10},
	urldate = {2018-11-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mazzoni, P. and Andersen, R. A. and Jordan, M. I.},
	month = may,
	year = {1991},
	pmid = {1903542},
	pages = {4433--4437},
	file = {Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\ETS5AVTH\\Mazzoni et al. - 1991 - A more biologically plausible learning rule for ne.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\V2E6RHQQ\\4433.html:text/html}
}

@article{maass_real-time_2002,
	title = {Real-{Time} {Computing} {Without} {Stable} {States}: {A} {New} {Framework} for {Neural} {Computation} {Based} on {Perturbations}},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976602760407955},
	doi = {10.1162/089976602760407955},
	language = {en},
	number = {11},
	urldate = {2018-11-08},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	pages = {2531--2560},
	file = {Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\CM7SZL2A\\Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf:application/pdf}
}

@article{friederich_fly_2016,
	title = {Fly {Photoreceptors} {Encode} {Phase} {Congruency}},
	volume = {11},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0157993},
	doi = {10.1371/journal.pone.0157993},
	abstract = {More than five decades ago it was postulated that sensory neurons detect and selectively enhance behaviourally relevant features of natural signals. Although we now know that sensory neurons are tuned to efficiently encode natural stimuli, until now it was not clear what statistical features of the stimuli they encode and how. Here we reverse-engineer the neural code of Drosophila photoreceptors and show for the first time that photoreceptors exploit nonlinear dynamics to selectively enhance and encode phase-related features of temporal stimuli, such as local phase congruency, which are invariant to changes in illumination and contrast. We demonstrate that to mitigate for the inherent sensitivity to noise of the local phase congruency measure, the nonlinear coding mechanisms of the fly photoreceptors are tuned to suppress random phase signals, which explains why photoreceptor responses to naturalistic stimuli are significantly different from their responses to white noise stimuli.},
	language = {en},
	number = {6},
	urldate = {2018-11-08},
	journal = {PLOS ONE},
	author = {Friederich, Uwe and Billings, Stephen A. and Hardie, Roger C. and Juusola, Mikko and Coca, Daniel},
	month = jun,
	year = {2016},
	keywords = {Frequency response, Histamine, Nonlinear dynamics, Photoreceptors, Sensory neurons, Signal to noise ratio, Vision, White noise},
	pages = {e0157993},
	file = {Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\DRJ9WJ5S\\Friederich et al. - 2016 - Fly Photoreceptors Encode Phase Congruency.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\BAGPL576\\article.html:text/html}
}

@article{song_stochastic_2012,
	title = {Stochastic, {Adaptive} {Sampling} of {Information} by {Microvilli} in {Fly} {Photoreceptors}},
	volume = {22},
	issn = {0960-9822},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3420010/},
	doi = {10.1016/j.cub.2012.05.047},
	abstract = {► Results define how microvilli populations in fly photoreceptors encode information ► Availability and response waveforms of microvilli map light changes to information ► Encoding mechanisms understood through simple adaptive sampling principles ► Sampling principles predict photoreceptor structure and encoding in other species},
	number = {15},
	urldate = {2018-11-08},
	journal = {Current Biology},
	author = {Song, Zhuoyi and Postma, Marten and Billings, Stephen A. and Coca, Daniel and Hardie, Roger C. and Juusola, Mikko},
	month = aug,
	year = {2012},
	pmid = {22704990},
	pmcid = {PMC3420010},
	pages = {1371--1380}
}

@article{bohte_unsupervised_2002,
	title = {Unsupervised clustering with spiking neurons by sparse temporal coding and multilayer {RBF} networks},
	volume = {13},
	issn = {1045-9227},
	doi = {10.1109/72.991428},
	abstract = {We demonstrate that spiking neural networks encoding information in the timing of single spikes are capable of computing and learning clusters from realistic data. We show how a spiking neural network based on spike-time coding and Hebbian learning can successfully perform unsupervised clustering on real-world data, and we demonstrate how temporal synchrony in a multilayer network can induce hierarchical clustering. We develop a temporal encoding of continuously valued data to obtain adjustable clustering capacity and precision with an efficient use of neurons: input variables are encoded in a population code by neurons with graded and overlapping sensitivity profiles. We also discuss methods for enhancing scale-sensitivity of the network and show how the induced synchronization of neurons within early RBF layers allows for the subsequent detection of complex clusters.},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bohte, S. M. and Poutre, H. La and Kok, J. N.},
	month = mar,
	year = {2002},
	keywords = {Neurons, artificial neural networks, Biological information theory, cortical neurons, Encoding, hierarchical clustering, Input variables, learning clusters, Multi-layer neural network, multilayer network, Neural networks, Nonhomogeneous media, pattern clustering, radial basis function networks, Radial basis function networks, RBF networks, spiking neural networks, synchronous firing, temporal coding, temporal synchrony, Timing, unsupervised learning, Very large scale integration},
	pages = {426--435},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Caleb\\Zotero\\storage\\HTM8EIAC\\991428.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\V799UB9W\\Bohte et al. - 2002 - Unsupervised clustering with spiking neurons by sp.pdf:application/pdf}
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {0893-6080},
	shorttitle = {Networks of spiking neurons},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	number = {9},
	urldate = {2018-11-08},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	month = dec,
	year = {1997},
	keywords = {Computational complexity, Integrate-and-fire neutron, Lower bounds, Sigmoidal neural nets, Spiking neuron},
	pages = {1659--1671},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\D8DQFVUX\\Maass - 1997 - Networks of spiking neurons The third generation .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\4VY7H6I2\\S0893608097000117.html:text/html}
}

@article{lukosevicius_reservoir_2009,
	title = {Reservoir computing approaches to recurrent neural network training},
	volume = {3},
	issn = {1574-0137},
	url = {http://www.sciencedirect.com/science/article/pii/S1574013709000173},
	doi = {10.1016/j.cosrev.2009.03.005},
	abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current “brand-names” of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed “map” of it.},
	number = {3},
	urldate = {2018-11-01},
	journal = {Computer Science Review},
	author = {Lukoševičius, Mantas and Jaeger, Herbert},
	month = aug,
	year = {2009},
	pages = {127--149},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\8UKKX7YH\\Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\7MLMEZFQ\\S1574013709000173.html:text/html}
}

@article{schrauwen_overview_nodate,
	title = {An overview of reservoir computing: theory, applications and implementations},
	abstract = {Training recurrent neural networks is hard. Recently it has however been discovered that it is possible to just construct a random recurrent topology, and only train a single linear readout layer. State-ofthe-art performance can easily be achieved with this setup, called Reservoir Computing. The idea can even be broadened by stating that any high dimensional, driven dynamic system, operated in the correct dynamic regime can be used as a temporal ‘kernel’ which makes it possible to solve complex tasks using just linear post-processing techniques.},
	language = {en},
	author = {Schrauwen, Benjamin},
	pages = {12},
	file = {Schrauwen - An overview of reservoir computing theory, applic.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\DYFHX3FF\\Schrauwen - An overview of reservoir computing theory, applic.pdf:application/pdf}
}

@misc{noauthor_neural_nodate,
	title = {Neural {Micro} circuits},
	note = "{http://www.lsm.tugraz.at/}",
	urldate = {2018-12-02},
	file = {Neural Micro circuits:C\:\\Users\\Caleb\\Zotero\\storage\\JHIAWMJT\\www.lsm.tugraz.at.html:text/html}
}

@article{auer_p-delta_nodate,
	title = {The p-{Delta} {Learning} {Rule} for {Parallel} {Perceptrons}},
	abstract = {A learning algorithm is presented for circuits consisting of a single layer of perceptrons (= threshold gates, or equivalently gates with a Heaviside activation function). We refer to such circuits as parallel perceptrons. In spite of their simplicity, these circuits can compute any boolean function if one views the majority of the binary perceptron outputs as the binary outputs of the parallel perceptron, and they are universal approximators for arbitrary continuous functions with values in [0, 1] if one views the fraction of perceptrons that output 1 as the analog output of the parallel perceptron. For a long time one has thought that there exists no competitive learning algorithms for these extremely simple circuits consisting of gates with binary outputs, which also became known as committee machines. It is commonly believed that one ∗Research for this article was partially supported by the ESPRIT Working Group NeuroCOLT, No. 8556, and the Fonds zur Fo¨rderung der wissenschaftlichen Forschung (FWF), Austria, project P12153.},
	language = {en},
	author = {Auer, Peter and Burgsteiner, Harald M and Maass, Wolfgang},
	pages = {29},
	file = {Auer et al. - The p-Delta Learning Rule for Parallel Perceptrons.pdf:C\:\\Users\\Caleb\\Zotero\\storage\\IQ27ILS2\\Auer et al. - The p-Delta Learning Rule for Parallel Perceptrons.pdf:application/pdf}
}

@article{watts_collective_1998,
	title = {Collective dynamics of ‘small-world’ networks},
	volume = {393},
	copyright = {1998 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/30918},
	doi = {10.1038/30918},
	abstract = {Networks of coupled dynamical systems have been used to model biological oscillators1,2,3,4, Josephson junction arrays5,6, excitable media7, neural networks8,9,10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks ‘rewired’ to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them ‘small-world’ networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
	language = {en},
	number = {6684},
	urldate = {2018-12-03},
	journal = {Nature},
	author = {Watts, Duncan J. and Strogatz, Steven H.},
	month = jun,
	year = {1998},
	pages = {440--442},
	file = {Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\M95XXTFA\\Watts and Strogatz - 1998 - Collective dynamics of ‘small-world’ networks.pdf:application/pdf;Snapshot:C\:\\Users\\Caleb\\Zotero\\storage\\GVV5YKNN\\30918.html:text/html}
}

@article{yamamoto_wiring_2002,
	title = {Wiring of the brain by a range of guidance cues},
	volume = {68},
	issn = {0301-0082},
	abstract = {During development of the central nervous system, growth cones navigate along specific pathways, recognize their targets and then form synaptic connections by elaborating terminal arbors. To date, a number of developmental and in vitro studies have characterized the nature of the guidance cues that underlie various types of axonal behavior, from initial outgrowth to synapse formation, including pathway selection, polarized growth, orientated growth, termination and branching. New approaches in molecular biology have identified several types of guidance cues, most of which are likely to act as local cues. Moreover, recent studies have indicated that axonal responsiveness to guidance cues changes dynamically, which appears to be elicited by environmental factors encountered by the navigating growth cones. This article addresses what molecular cues are responsible for guidance mechanisms including axonal responsiveness, focusing on axonal behavior in the developmental stages.},
	language = {eng},
	number = {6},
	journal = {Progress in Neurobiology},
	author = {Yamamoto, Nobuhiko and Tamada, Atsushi and Murakami, Fujio},
	month = dec,
	year = {2002},
	pmid = {12576293},
	keywords = {Neurons, Synapses, Axons, Brain, Cell Communication, Cell Differentiation, Growth Cones, Nerve Growth Factors, Nerve Tissue Proteins, Neural Pathways, Neuronal Plasticity, Signal Transduction},
	pages = {393--407}
}

@article{morante_color_2008,
	title = {The color vision circuit in the medulla of {Drosophila}},
	volume = {18},
	issn = {0960-9822},
	note = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2430089/},
	doi = {10.1016/j.cub.2008.02.075},
	abstract = {Background
Color vision requires comparison between photoreceptors that are sensitive to different wavelengths of light. In Drosophila, this is achieved by the inner photoreceptors (R7 and R8) that contain different rhodopsins. Two types of comparisons can occur in fly color vision: between the R7 (UV-sensitive) and R8 (blue or green-sensitive) photoreceptor cells within one ommatidium (unit eye); or between different ommatidia that contain spectrally distinct inner photoreceptors. Photoreceptors project to the optic lobes: R1-6, which are involved in motion detection, project to the lamina, while R7 and R8 reach deeper in the medulla. This paper analyzes the neural network underlying color vision in the medulla.

Results
We reconstruct the neural network in the medulla, focusing on neurons likely to be involved in processing color vision. We identify the full complement of neurons in the medulla, including second order neurons that contact both R7 and R8 from a single ommatidium, or contact R7 and/or R8 from different ommatidia. We also examine third order neurons and local neurons that likely modulate information from second order neurons. Finally, we present highly specific tools that will allow us to functionally manipulate the network and test both activity and behavior.

Conclusions
This precise characterization of the medulla circuitry will allow us to understand how color vision is processed in the optic lobe of Drosophila, providing a paradigm for more complex systems in vertebrates.},
	number = {8},
	urldate = {2018-12-03},
	journal = {Current biology : CB},
	author = {Morante, Javier and Desplan, Claude},
	month = apr,
	year = {2008},
	pmid = {18403201},
	pmcid = {PMC2430089},
	pages = {553--565},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\2PRIZ8R2\\Morante and Desplan - 2008 - The color vision circuit in the medulla of Drosoph.pdf:application/pdf}
}

@article{zhu_drosophila_2013-1,
	title = {The {Drosophila} visual system},
	volume = {7},
	issn = {1933-6918},
	note= {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3739809/},
	doi = {10.4161/cam.25521},
	abstract = {A compact genome and a tiny brain make Drosophila the prime model to understand the neural substrate of behavior. The neurogenetic efforts to reveal neural circuits underlying Drosophila vision started about half a century ago, and now the field is booming with sophisticated genetic tools, rich behavioral assays, and importantly, a greater number of scientists joining from different backgrounds. This review will briefly cover the structural anatomy of the Drosophila visual system, the animal’s visual behaviors, the genes involved in assembling these circuits, the new and powerful techniques, and the challenges ahead for ultimately identifying the general principles of biological computation in the brain.
, 
 , 
A typical brain utilizes a great many compact neural circuits to collect and process information from the internal biological and external environmental worlds and generates motor commands for observable behaviors. The fruit fly Drosophila melanogaster, despite of its miniature body and tiny brain, can survive in almost any corner of the world.1 It can find food, court mate, fight rival conspecific, avoid predators, and amazingly fly without crashing into trees. Drosophila vision and its underlying neuronal machinery has been a key research model for at least half century for neurogeneticists.2 Given the efforts invested on the visual system, this animal model is likely to offer the first full understanding of how visual information is computed by a multi-cellular organism. Furthermore, research in Drosophila has revealed many genes that play crucial roles in the formation of functional brains across species. The architectural similarities between the visual systems of Drosophila and vertebrate at the molecular, cellular, and network levels suggest new principles discovered at the circuit level on the relationship between neurons and behavior in Drosophila shall also contribute greatly to our understanding of the general principles for how bigger brains work.3 I start with the anatomy of Drosophila visual system, which surprisingly still contains many uncharted areas.},
	number = {4},
	urldate = {2018-12-03},
	journal = {Cell Adhesion \& Migration},
	author = {Zhu, Yan},
	month = jul,
	year = {2013},
	pmid = {23880926},
	pmcid = {PMC3739809},
	pages = {333--344},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Caleb\\Zotero\\storage\\T7UAN8VI\\Zhu - 2013 - The Drosophila visual system.pdf:application/pdf}
}